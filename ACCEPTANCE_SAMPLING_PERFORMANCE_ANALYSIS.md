# Acceptance Sampling API Performance Analysis
**Date:** October 6, 2025 - 5:30 AM EST
**Issue:** API timeout (>10 seconds)
**Target:** <3 seconds response time

---

## üîç PERFORMANCE BOTTLENECK IDENTIFIED

### Critical Code Section
**File:** `/backend/sqc_analysis/services/acceptance_sampling_service.py`
**Method:** `calculate_single_sampling_plan()` (lines 25-157)
**Issue:** Nested loop with expensive binomial PMF calculations

### Problematic Code (Lines 64-106)

```python
# Search range for n and c
n_range = range(max(5, n_guess // 2), min(n_guess * 2, lot_size) + 1)

for n in n_range:  # FIRST LOOP: 50-200 iterations
    # Calculate acceptance probability at p1 (AQL)
    pa_p1 = sum(stats.binom.pmf(i, n, p1) for i in range(n + 1))

    # Start with c at max(1, n*p1) and decrease to find optimal value
    for c in range(min(n, max(1, int(n * p1 * 2))), -1, -1):  # SECOND LOOP: 10-50 iterations
        # Calculate producer's and consumer's actual risks
        producer_actual_risk = 1 - sum(stats.binom.pmf(i, n, p1) for i in range(c + 1))  # EXPENSIVE
        consumer_actual_risk = sum(stats.binom.pmf(i, n, p2) for i in range(c + 1))  # EXPENSIVE

        # Check if both risks are satisfied
        if producer_actual_risk <= producer_risk and consumer_actual_risk <= consumer_risk:
            # ... update best plan ...

# If no plan satisfies both risks, REPEAT THE ENTIRE SEARCH AGAIN (lines 88-105)
if best_risk_diff == float('inf'):
    for n in n_range:  # DUPLICATE SEARCH
        for c in range(min(n, max(1, int(n * p1 * 2))), -1, -1):
            # ... same expensive calculations ...
```

---

## üìä PERFORMANCE METRICS

### Complexity Analysis

**For typical input:**
- lot_size = 1000
- n_guess = 100 (10% of lot size)
- n_range = range(50, 200) = **150 iterations**
- c_range average = **30 iterations** per n

**Total iterations:**
```
First search:  150 n √ó 30 c = 4,500 iterations
Second search: 150 n √ó 30 c = 4,500 iterations (if first fails)
TOTAL: 9,000 iterations
```

**Binomial PMF calls per iteration:**
```
Each iteration calculates:
- producer_actual_risk: sum(binom.pmf(...)) over c+1 values
- consumer_actual_risk: sum(binom.pmf(...)) over c+1 values
Average c = 15, so ~30 binom.pmf calls per iteration
```

**Total binom.pmf calls:**
```
9,000 iterations √ó 30 calls = 270,000 binom.pmf() calls
Each binom.pmf() call: ~10 microseconds
Total time: 270,000 √ó 10Œºs = 2.7 seconds (just for PMF)
Plus loop overhead, memory allocation: ~5-10 seconds
```

**Measured Performance:**
- Response time: >10 seconds ‚ùå
- Unacceptable for production use

---

## üéØ OPTIMIZATION STRATEGIES EVALUATED

### Option 1: Binary Search + Caching (RECOMMENDED)
**Approach:**
- Replace linear search for n with binary search
- Cache binom.pmf results within inner loop
- Add early termination when acceptable solution found
- Remove duplicate search (lines 88-105)

**Complexity Reduction:**
```
Current: O(n_range √ó c_range) = O(150 √ó 30) = O(4,500)
Optimized: O(log(n_range) √ó c_range) = O(8 √ó 30) = O(240)
Speedup: 4,500 / 240 = 18.75x
```

**Expected Performance:**
- Response time: <1 second ‚úÖ
- Error rate: 0% (same algorithm, just faster)
- Complexity: Low (2-3 hours implementation)

**Advantages:**
‚úÖ Fixes root cause
‚úÖ No infrastructure changes
‚úÖ Consistent performance
‚úÖ Easy to test
‚úÖ Maintains scientific accuracy

**Disadvantages:**
‚ö†Ô∏è Requires code changes
‚ö†Ô∏è Need thorough testing

---

### Option 2: Pre-computed Lookup Table
**Approach:**
- Pre-compute sampling plans for common (AQL, LTPD) combinations
- Store in Django model or JSON file
- Return immediately if exact match found
- Fall back to calculation for custom values

**Expected Performance:**
- Response time (cached): <0.1 second ‚úÖ
- Response time (not cached): >10 seconds ‚ùå
- Hit rate: ~70-80% (estimated)

**Advantages:**
‚úÖ Extremely fast for cached values
‚úÖ No algorithm changes for fallback

**Disadvantages:**
‚ùå Doesn't fix root cause
‚ùå Unpredictable performance (cached vs not)
‚ùå Maintenance burden (keep lookup table updated)
‚ùå Storage overhead
‚ùå Still slow for custom values

---

### Option 3: Async with Celery
**Approach:**
- Move calculation to background Celery task
- Return job_id immediately
- Poll for results via separate endpoint

**Expected Performance:**
- Initial response: <0.1 second ‚úÖ
- Full calculation: >10 seconds ‚ùå
- User experience: Polling complexity

**Advantages:**
‚úÖ Immediate response
‚úÖ Doesn't block server

**Disadvantages:**
‚ùå Doesn't fix root cause
‚ùå Adds infrastructure complexity (Celery, Redis)
‚ùå Complex frontend integration (polling)
‚ùå Still slow calculation time
‚ùå 3-4 hours implementation

---

## ‚úÖ DECISION: OPTION 1 (Binary Search + Caching)

### Rationale
1. **Fixes root cause:** Reduces algorithmic complexity from O(n¬≤) to O(n log n)
2. **Fastest implementation:** 2-3 hours vs 3-4 hours for Celery
3. **No infrastructure changes:** Works with existing Django setup
4. **Consistent performance:** Always fast, not just for cached values
5. **Easy to test:** Same inputs, same outputs, just faster
6. **Maintains accuracy:** No algorithmic changes, just optimization

---

## üîß IMPLEMENTATION PLAN

### Step 1: Optimize Sample Size Search (Binary Search)

**Current (Linear Search):**
```python
n_range = range(max(5, n_guess // 2), min(n_guess * 2, lot_size) + 1)
for n in n_range:
    # ... test each n value ...
```

**Optimized (Binary Search):**
```python
def binary_search_n(min_n, max_n, p1, p2, producer_risk, consumer_risk):
    """Binary search for optimal sample size"""
    best_n, best_c, best_diff = None, None, float('inf')

    # Test powers of 2 for faster convergence
    n_candidates = []
    n = min_n
    while n <= max_n:
        n_candidates.append(n)
        n = int(n * 1.5)  # Geometric progression instead of linear
    n_candidates.append(max_n)

    for n in n_candidates:
        # ... search for best c for this n ...

    return best_n, best_c
```

**Reduction:** 150 iterations ‚Üí ~15 iterations (10x faster)

---

### Step 2: Cache Binomial PMF Results

**Current (Recalculates Every Time):**
```python
producer_actual_risk = 1 - sum(stats.binom.pmf(i, n, p1) for i in range(c + 1))
consumer_actual_risk = sum(stats.binom.pmf(i, n, p2) for i in range(c + 1))
```

**Optimized (Cache Cumulative Sum):**
```python
# Pre-compute cumulative binomial probabilities for each n
def get_cumulative_binom(n, p, max_c):
    """Pre-compute cumulative binomial probabilities"""
    cum_prob = np.zeros(max_c + 1)
    for i in range(max_c + 1):
        cum_prob[i] = sum(stats.binom.pmf(j, n, p) for j in range(i + 1))
    return cum_prob

# Use cached values
cum_p1 = get_cumulative_binom(n, p1, n)
cum_p2 = get_cumulative_binom(n, p2, n)

for c in range(...):
    producer_actual_risk = 1 - cum_p1[c]
    consumer_actual_risk = cum_p2[c]
```

**Reduction:** 30 PMF calls per iteration ‚Üí 1 pre-computation per n (30x faster per iteration)

---

### Step 3: Add Early Termination

**Current (Searches Entire Range):**
```python
for n in n_range:
    for c in range(...):
        # ... always completes full search ...
```

**Optimized (Stop When Good Enough):**
```python
# Define "good enough" threshold
ACCEPTABLE_RISK_TOLERANCE = 0.001

for n in n_candidates:
    for c in range(...):
        if risk_diff < ACCEPTABLE_RISK_TOLERANCE:
            return best_n, best_c  # Early exit
```

**Reduction:** May exit after 10-20% of search space

---

### Step 4: Remove Duplicate Search

**Current (Lines 88-105):**
```python
# If no plan satisfies both risks, find the closest compromise
if best_risk_diff == float('inf'):
    # DUPLICATE SEARCH - same loops again
    for n in n_range:
        for c in range(...):
            # ... same calculations ...
```

**Optimized (Single Combined Search):**
```python
# Combined search - no duplication
for n in n_candidates:
    for c in range(...):
        # Check if satisfies both risks OR is best compromise
        if satisfies_both OR (risk_diff < best_risk_diff):
            best_n, best_c = n, c
```

**Reduction:** Eliminates 50% of computation

---

## üìà EXPECTED PERFORMANCE IMPROVEMENT

### Before Optimization
```
Linear search:       150 iterations for n
PMF recalculation:   30 calls √ó 4,500 iterations = 135,000 calls
Duplicate search:    2√ó (double the work)
Early termination:   None
Total time:          >10 seconds
```

### After Optimization
```
Binary search:       ~15 iterations for n (10x reduction)
PMF caching:         Pre-computed, O(1) lookup (30x reduction)
Duplicate search:    Eliminated (2x reduction)
Early termination:   May exit early (1.5x average reduction)
Combined speedup:    10 √ó 30 √ó 2 √ó 1.5 = 900x
Expected time:       >10s / 900 = ~0.01 seconds
```

**Conservative Estimate:** 0.5-1 second (accounting for overhead, network, etc.)

**Realistic Target:** <3 seconds ‚úÖ

---

## üß™ TESTING PLAN

### Unit Tests
```python
def test_optimized_vs_original():
    """Verify optimized version produces same results"""
    lot_size = 1000
    aql = 0.01
    ltpd = 0.05

    # Original (slow)
    result_original = calculate_single_sampling_plan_original(lot_size, aql, ltpd)

    # Optimized (fast)
    result_optimized = calculate_single_sampling_plan_optimized(lot_size, aql, ltpd)

    # Results should be identical (or very close)
    assert abs(result_original['sample_size'] - result_optimized['sample_size']) <= 1
    assert result_original['acceptance_number'] == result_optimized['acceptance_number']
```

### Performance Tests
```python
import time

def test_performance():
    """Verify performance improvement"""
    lot_size = 1000
    aql = 0.01
    ltpd = 0.05

    start = time.time()
    result = calculate_single_sampling_plan(lot_size, aql, ltpd)
    elapsed = time.time() - start

    assert elapsed < 3.0, f"Performance test failed: {elapsed:.2f}s > 3.0s"
    print(f"‚úÖ Performance test passed: {elapsed:.2f}s")
```

### API Tests
```bash
# Test with curl (should complete in <3 seconds)
time curl -X POST http://localhost:8000/api/v1/sqc-analysis/quick-sampling/ \
  -H "Content-Type: application/json" \
  -d '{
    "lot_size": 1000,
    "aql": 0.01,
    "ltpd": 0.05,
    "producer_risk": 0.05,
    "consumer_risk": 0.10
  }'
```

---

## ‚úÖ SUCCESS CRITERIA

### Performance
- [ ] Response time <3 seconds (target: <1 second)
- [ ] Error rate <1%
- [ ] Results match original algorithm (within rounding errors)

### Quality
- [ ] Unit tests pass
- [ ] Performance tests pass
- [ ] API tests pass
- [ ] No regressions in other methods

### Integration
- [ ] Frontend integration works
- [ ] Browser testing passes
- [ ] Load testing with 50 concurrent users passes

---

## üìù IMPLEMENTATION CHECKLIST

- [ ] Create backup of original method (for comparison testing)
- [ ] Implement binary search for n
- [ ] Implement PMF caching
- [ ] Add early termination logic
- [ ] Remove duplicate search
- [ ] Add performance timing logs
- [ ] Write unit tests
- [ ] Write performance tests
- [ ] Test with curl
- [ ] Commit with detailed message

---

## üéì KEY LEARNINGS

### What Caused the Timeout
1. **Nested loops:** O(n¬≤) complexity
2. **Redundant calculations:** PMF recalculated every iteration
3. **No early termination:** Always searches entire space
4. **Duplicate search:** Same work done twice if first fails

### How Optimization Works
1. **Binary search:** Reduces search space exponentially
2. **Caching:** Eliminates redundant calculations
3. **Early termination:** Stops when good solution found
4. **Single pass:** No duplicate work

### Why This Will Work
1. **Algorithmic improvement:** Fundamentally better approach
2. **Proven technique:** Binary search is well-established
3. **Maintains accuracy:** No approximations or shortcuts
4. **Easy to test:** Same inputs ‚Üí same outputs ‚Üí just faster

---

**Next Step:** Implement the optimization following this plan
**Estimated Time:** 2-3 hours
**Expected Result:** <1 second response time ‚úÖ
